{"cells":[{"cell_type":"markdown","source":["<h2>INTRODUCTION TO PARTITION</h2>\n\nSpark is one of the distributed computing platform that split data into partitions to achieve parallel computation. The challange with these distributed computing platforms is to manage these partition to keep your spark computations running efficiently. In this topic we discuss how to manage the spark partition with repartition and coalesce."],"metadata":{}},{"cell_type":"code","source":["#Let's create a dataframe to illustrate how data is partitioned.\nds = spark.range(1, 11)\nds = ds.withColumnRenamed(\"id\", \"num_idb\")\ndisplay(ds)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_idb</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr><tr><td>9</td></tr><tr><td>10</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["\"\"\"\nCheck the number of parition\n\"\"\"\n\nds.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>8\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["\"\"\"\nGet the parition distribution\n\"\"\"\nfrom pyspark.sql.functions import spark_partition_id\n\nds.select(*ds.columns, spark_partition_id().alias(\"pid\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+---+\nnum_id|pid|\n+------+---+\n     1|  0|\n     2|  1|\n     3|  2|\n     4|  3|\n     5|  3|\n     6|  4|\n     7|  5|\n     8|  6|\n     9|  7|\n    10|  7|\n+------+---+\n\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["<h2>coalesce</h2>\nThe coalesce method reduces the number of partitions in a DataFrame. Here’s how to consolidate the data in two partitions:"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\nUse colaesce function to reduce the number of parition.In this case we are reducing to 2 partition.\nAnd also verifying the partition distribution after the coalesce.\n\"\"\"\ndsCoalesce = ds.coalesce(2)\ndsCoalesce.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">10</span><span class=\"ansired\">]: </span>2\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import spark_partition_id\ndsCoalesce.select(*dsCoalesce.columns, spark_partition_id().alias(\"pid\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---+\nnum_idb|pid|\n+-------+---+\n      1|  0|\n      2|  0|\n      3|  0|\n      4|  0|\n      5|  0|\n      6|  1|\n      7|  1|\n      8|  1|\n      9|  1|\n     10|  1|\n+-------+---+\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["<h2>Increasing partitions using coalesce</h2>\nYou can try to increase the number of partitions with coalesce, but it won’t work!"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\nUse colaesce function to increase the number of parition and see it won't effect the partitions.\n\"\"\"\nfrom pyspark.sql.functions import spark_partition_id\n\ndsCoalesceInc = ds.coalesce(10)\n\ndsCoalesceInc.rdd.getNumPartitions()\n\n#dsCoalesceInc.select(*dsCoalesceInc.columns, spark_partition_id().alias(\"pid\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>8\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["<h2>Repartition</h2>\nThe repartition method can be used to either increase or decrease the number of partitions in a DataFrame."],"metadata":{}},{"cell_type":"code","source":["\"\"\"\nLet's create with repartition on ds dataframe\n\"\"\"\ndsRepartition = ds.repartition(2)\ndsRepartition.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">12</span><span class=\"ansired\">]: </span>2\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import spark_partition_id\ndsRepartition.select(*dsRepartition.columns, spark_partition_id().alias(\"pid\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---+\nnum_idb|pid|\n+-------+---+\n      1|  0|\n      3|  0|\n      6|  0|\n      8|  0|\n      2|  0|\n      5|  0|\n      7|  0|\n     10|  0|\n      4|  1|\n      9|  1|\n+-------+---+\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["<h2>Increasing partitions using repartition</h2>\nRepartition will be used to increase the number of partition."],"metadata":{}},{"cell_type":"code","source":["\"\"\"\nIncrease the partition size.\n\"\"\"\ndsRepartitionInc = ds.repartition(10)\ndsRepartitionInc.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>10\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.functions import spark_partition_id\ndsRepartitionInc.select(*dsRepartitionInc.columns, spark_partition_id().alias(\"pid\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---+\nnum_idb|pid|\n+-------+---+\n      1|  1|\n      8|  2|\n      6|  3|\n      5|  5|\n      2|  6|\n      4|  6|\n     10|  7|\n      7|  8|\n      9|  8|\n      3|  9|\n+-------+---+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["\"\"\"\nThe above results showing only the assigned partition number but not include any empty parition in the results.\nIf you need the entire structure of the partition that includes the empty partitions use the below code.\n\"\"\"\nprint('Partitions structure: {}'.format(dsRepartitionInc.rdd.glom().collect()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Partitions structure: [[], [Row(num_idb=1)], [Row(num_idb=8)], [Row(num_idb=6)], [], [Row(num_idb=5)], [Row(num_idb=2), Row(num_idb=4)], [Row(num_idb=10)], [Row(num_idb=7), Row(num_idb=9)], [Row(num_idb=3)]]\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["<h2>Differences between coalesce and repartition</h2>\nThe repartition algorithm does a full shuffle of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a full shuffle."],"metadata":{}}],"metadata":{"name":"10-Partitioning-Performance","notebookId":3923461933236601},"nbformat":4,"nbformat_minor":0}
